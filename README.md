
# FFL: Fine-grained Fault Localization for Student Programs via Syntactic and Semantic Reasoning

## Abstract
Fault localization has been used to provide feedback for incorrect student programs since locations of faults can be a valuable hint for students about what caused their programs to crash. Unfortunately, existing fault localization techniques for student programs are limited because they usually consider either the program's syntax or semantics alone. This motivates the new design of fault localization techniques that use both semantic and syntactical information of the program.In this paper, we introduce FFL (Fine grained Fault Localization), a novel technique using syntactic and semantic reasoning for localizing bugs in student programs. The novelty in FFL that allows it to capture both syntactic and semantic of a program is three-fold: (1) A fine-grained graph-based representation of a program that is adaptive for statement-level fault localization; (2) an effective and efficient model to leverage the designed representation for fault-localization task and (3) a node-level training objective that allows deep learning model to learn from fine-grained syntactic patterns. We compare FFL's effectiveness with state-of-the-art fault localization techniques for student programs (NBL, Tarantula, Ochiai and DStar) on two real-world datasets: Prutor and Codeflaws. Experimental results show that FFL successfully localizes bug for 84.6% out of 2136 programs on Prutor and 83.1% out of 780 programs on Codeflaws concerning the top-10 suspicious statements. FFL also remarkably outperforms the best baselines by 197%, 104%, 70%, 22% on Codeflaws dataset and 10%, 17%, 15% and 8% on Prutor dataset, in term of top-1, top-3, top-5, top-10, respectively.

## What is Fault Localization?
Fault localization is the problem of identifying faulty lo-cations in source code which leads to erroneous behaviorstriggered when running a test suite. Due to large variabilityof faulty causes, it is a challenging problem to narrow downthe possible root causes from the triggered failure. This isespecially hard for students, where they may have little famil-iarity with the programming language for identifying faultylocations as well as root causes.

## What we did
We introduce FFL (Fine grained FaultLocalization), a novel technique using syntactic and seman-tic reasoning for localizing bugs in student programs. FFL utilizes both syntactic and semantic information of the pro-gram for fault localization via our new design of three maincomponents: (1) graph-based representation, namely syntax-coverage graph, of a program that comprises syntax andprogram semantic information via Abstract Syntax Tree and detailed coverage of given tests into one graph; (2) an effectiveand efﬁcient deep learning model (i.e., graph neural network(GNN)) which is able to naturally deal with graph-basedrepresentations; and (3) a node-level training objective thatallows deep learning model to learn from ﬁne-grained syntax patterns. coverage graph. In the second phase, we ﬁrst leverage a graphneural network to predict suspiciousness scores at AST nodelevel (i.e., the probability of each AST node being modiﬁed toﬁx bugs). We then aggregate AST-node-level results to obtainthe statement-level faulty score.

### Our Approach
III. In this section, we introduce a deep learning model oversyntactic and semantic features for fault localization in student programs. Toward this goal, we describe a graph-based rep-resentation comprising of both program syntax (i.e., AbstractSyntax Tree) and semantic features (i.e., coverage informa-tion) with a tailored graph neural network (GNN) to identifybuggy locations based on this representation. The key ideais that by leveraging the graph-based representation, we cantreat fault localization problems as node classiﬁcation on agraph, in which we predict whether each node in a graph is erroneous. Furthermore, while aiming for statement-level fault localization is a straightforward option, we hypothesize that anode-level feedback signal would boost model performance.Thus, we design a training objective based on node-level ASTdifferencing as the label for graph node classiﬁcation.A. Syntactic and Semantic Program RepresentationGNN is widely known for its effectiveness in dealing withstructured data. However, in order to leverage the power of GNN, designing an expressive representation of the input datais crucial.To achieve this, we propose to use both syntax and semantic information to build a graph-based representation of inputprograms that empowers GNN to learn effectively. 

Finally, we obtain a representation that combines bothsyntactic and semantic information, namely syntax-coveragegraph. We lay out the details of our syntax-coverage graphconstruction below.1) Syntactic Representation via Abstract Syntax Tree: Inthe syntax-coverage graph, we incorporate program syntax byleveraging AST as a subgraph in our representation. In detail,given an abstract syntax tree in the form of a graph Gast =(Vast, East ), where each node v∈V represents a node in the abstract syntax tree, a directed edge (vi, vj)∈East exists forevery parent-child in the AST, we incorporate every node inthe abstract syntax tree as a part of our syntax-coverage graph(i.e. Vast ⊆VHand East ⊆EH). As an illustration, ASTnodes are represented as ellipse-shaped nodes in Figure 3.2) Semantic Representation via coverage information: Asanother part of the syntax-semantic representation, we includesemantic features via test coverage information. Given a testsuite T={t1, t2, t3,...tNT}where tifor i∈1..NTisa test case, we represent each test case as a node in thesyntax-coverage graph. In order to enrich the syntax-coveragerepresentation, we obtain the outcome for each of these testcases by running the test suite and embed the outcome of intotheir corresponding edges between their test nodes and thecovered AST nodes: For the example in Figure 3), since testt0’s outcome is passed, its connections towards the coveredAST nodes are passing edges (dotted edges), for the failed testt1, the edges will be of failing type (dashed edges in Figure 3).This aids the GNN model in distinguishing test case types (i.e.passed or failed) and aggregating this information towards itscovered AST node. Subsequently, this representation allowsFFL learning to make use of both faulty syntax patternsand syntax-coverage patterns that frequently presents in thedataset.

### The model that was built
An overview of FFL’s architecture is shown in Figure 2.FFL works in two phases. In the training phase, it learns a deep learning model to determine whether each AST node in the syntax-coverage graph is faulty. The training data consists of a set of historical bugs, consisting of a buggy program, a setof tests (failing and passing), and ground truth bug locations.The training phase of FFL consists of two main steps:•Input Preparation. (Section III-B1) FFL ﬁrst uses ASTparser and coverage analysis tool to produce AST tree and coverage information of program. Then, it uses agraph builder to construct syntax-coverage graph of inputprogram.•Node classiﬁcation via GNN. (Section III-B2 and III-B3)FFL takes the syntax-coverage graphs and the ground truth locations to train a graph neural networks that determines whether a syntax-coverage graph’s AST node is faulty. This model is the overall output of the training phase that is passed to the deployment phase.In the deployment phase (Section III-B4), FFL takes asinput a set of test cases (including both failing and passing tests) and a buggy program, and constructs syntax-coverage graphs through input preparation. After that, FFL uses thepre-trained model to produce a suspiciousness score for each node of syntax-coverage graphs. Then, FFL computes the suspiciousness score of a statement by aggregating the score ofeach node that belongs to the statement. Finally, FFL produces a ranked list of statements that are likely responsible for the failing test cases. 1) Input Preparation: Given a buggy program and a setof tests (passing and failing), FFL constructs syntax-coverage graph Gas follows. We ﬁrst parse the buggy program by pycparser 2to obtain AST representations Gast for theprogram. Simultaneously, we run the buggy program over thegiven tests and perform coverage analysis by using gcov 3. to obtain coverage information.Note that, gcov only provides coverage information at statement level. Hence, we associate the line-level coverage information obtained by gcov to the AST nodes by connectingeach node in a statement to test cases that cover the statement.Then, FFL connects Gast of each program into one graphby including test nodes and coverage edges according tocoverage information. Finally, we annotate each node andedge with its attributes in the graph. As mentioned in SectionIII-A, we annotate nodes and edges of AST following its type generated by pycparser. Meanwhile, test nodes and coverage edges types are determined based on test outcomes. More speciﬁcally, the edge will be of passing type if the test outcome is passed; otherwise, the edge will be of failing type.2) Graph Neural Network Architechture: Given heteroge-neous syntax-coverage graph GH= (VH, EH)where VH=Vast ∪Vtest and EH=East ∪Ecov. We proposed a GNN thattakes input as GHand provides a prediction label for eachnode v∈Vast. As input representation of GNN, we use an embedding layer to retrieve numerical feature representation of each node and each edge. This input is then fed through several message passing layers, with each layer updating each node feature. We take each node’s representation output ofthe last layer and calculate node label prediction. Finally, wea ggregate the node-level predictions to retrieve statement-level suspicious scores from them.

Embedding layer. Each node in the syntax-coverage graphshould be assigned with a corresponding node type t∈Tnodes where Tnodes is the set of all node labels (e.g.F uncCal l, If , T est etc. ), which we leverage to obtain thenumerical feature of each node in the input graph. In de-tail, each node type tis encoded using an one-hot vectorxt∈ {0,1}|Tnodes|, where each dimension in the vector is setto 1if the node is of corresponding type. We stack these nodefeatures to obtain the feature matrix X0H∈ {0,1}|VH|×|Tnodes|.Finally, we apply a linear transformation to X0Hto obtainhidden representation H(0) ∈RF(0) where F(0) is a chosenhyper-parameter for embedding. Each row of h(0)i=H(0)i,:corresponds to a node’s embedding.

Message-passing layer. Given node embedding, we leveragegraph input structure to update hidden node features throughlayer via widely-used message passing mechanism [11]. The speciﬁcally chosen message passing mechanism has to beﬂexible towards multiple types of edges. For this task, wechoose R-GCN [30] which has been known for its capability in dealing with heterogeneous graph, its form is shown below: Where lis the layer index, σis ReLU activation function[26], W(l)r,W(l)0∈RF(l),b(l)is layer l’s learnable parameter,RF(l)is a hyper parameter of choice and Nriis 1−hop neighbor having relation of type rwith the node of target. Intuitively, each node feature is updated in parallel. After passing through LGNN message passing layers [11],we obtain L-th hidden feature matrix of each node: H(L)∈RF(L)Output layer. Each of node’s classes probabilities are obtainedby applying a linear transformation followed by class-wises oft max:O=softmax WoH(L)+bo∈[0,1]|VH|×C(4)Where Wo and bo are learnable parameters and C is thenumber of classes. Each row oi=O[i, :] ∈RC represent each node’s probability of belong to each class. In our caseCis set to 3, with classes 0,1,2 correspond to unmodiﬁed,modiﬁed, and inserted respectively.3) Training objective: To prepare labeling for training, wemake use of Gumtree ﬁne-grained AST differencing algorithm[10]. Taking as input the source code of two versions, one buggy and one ﬁxed source code, Gumtree output mapping between AST nodes of the source code to a ﬁxed version.We determine the unmodiﬁed, modiﬁed, inserted nodes via the following procedure

- Modiﬁed nodes are either removed or changed in con-tent: In the former case, we search nodes having no correspondence from the buggy to ﬁxed version whereasto identify the latter, we ﬁnd nodes having its contentchanged between the buggy and ﬁxed version.
- 
- Inserted nodes are nodes that have no correspondencefrom the ﬁxed version to the buggy version.We perform automated annotation on the buggy source codeversion based on the aforementioned procedure. Inserted node is tricky to annotate due to its lack of appearance in the under-annotation buggy source AST; for this, we further use these annotations:•For token/expression-level modiﬁcation, we would takethe parent that has not been modiﬁed in the buggy versionand label the parent node as insertion.
- For statement-level insertion, we take the previous state-ment for annotation of modiﬁcation the common parent to be annotated as inserted With this, we obtain ﬁne-grained annotation for training and evaluation (see Figure 4 for an illustration).Since our ﬁnal objective is statement-level suspicious score,a straightforward approach might be performing classiﬁcationon statement nodes: where we classify whether each statement contains modiﬁed or inserted element. However, since we wish to learn ﬁne-grained AST transformation, we instead adopt node-level classiﬁcation to predict which of the aforementioned classes each node belong, then aggregate these detailed prediction to obtain statement-level prediction. We show the effectiveness of node-level classiﬁcation in comparison with statement-level classiﬁcation in Section IV-C.In order to classify each node to one of the aforementioned classes, cross entropy objective is a popular objective function,which we leverage to learn node-level classiﬁcation on eachgraph:Lθ(O,L) = −Xc∈Ci∈1..|VH|Li,c log oi,c (5)Where L∈0,1|VH|×Cis label annotation obtained from aforementioned annotation process for each node and oi,c is the probability of node i belonging to class c. For our main approach, class c can be either unmodiﬁed, modiﬁed or inserted and i indicate each node in AST Tree. For statement-level ver-sion used for comparison in Section IV-B, the class c is either unmodiﬁed or modiﬁed and i indicate each statement nodein the AST tree. As mentioned, since node-level predictionis different from our ﬁnal objective of statement-level faultlocalization, we introduce the process of obtaining statement-level fault localization from node-level prediction below.4) Applications: After the training phase, users can leverage the trained FFL to determine statements that are likely to be responsible for the failure of their programs. Given a student program that is failing in the grading system (i.e. itfails at least one test case), FFL ﬁrst constructs a syntax-coverage graph via input preparation and combines the trainedGNN model’s node level output wih a statement-level Ranking Model as follows to provide statement-level suspicious score:

- Node suspiciousness. In this step, FFL uses the trainedGNN model from training phase to produce a suspicious-ness score scoreifor a node iof the syntax-coveragegraph as follows:scorei= 1 −oi,0(6)Here, oi,0represents the probability that node ishouldbe unmodiﬁed.•Statement Prediction Aggregator. Since our target isstatement-level prediction, as our current GNN predictiontargeted AST nodes in general (i.e. expression/token arealso included), we introduce our method to convert fromthe AST-node prediction to the corresponding statement suspiciousness score. In detail, the suspiciousness scoreof a statement is calculated by taking the maximum ofall nodes in its correspoding subtree Sin AST. Moreformally, given a statement swith correspoding subtree Sin AST, we computed its suspiciousness score as follows:
## Results
We evaluated FFL on 2,136 buggy programs from thePrutor dataset [5] and 780 programs from Codeﬂaws dataset[33]. We compare FFL against the state-of-the-art spectrum-based and learning-based fault localization techniques forstudent programs, consisting of NBL [14], Tarantula [17],Ochiai [1] and DStar [38]. Experimental results show that FFLsuccessfully localizes bugs for 84.6% out of 2,136 programson Prutor and 83.1% out of 780 programs on Codeﬂaws whenreporting the top-10 suspicious lines. FFL also remarkably outperforms the best baselines by 197%, 104%, 70%, 22%on the Codeﬂaws dataset and 26%, 17%, 22% and 38% onthe Prutor dataset, in term of top-1,top-3,top-5,top-10. In summary, our contributions include:•We propose a novel technique, namely FFL, that is theﬁrst to combine syntactic and semantic information toautomatically localize bugs in student programs.•We propose syntax-coverage graph that can capture ﬁne-grained syntax-semantic representation of programs atAST node-level.•We design a graph-based deep learning model and anovel training objective to effectively and efﬁcientlylearn the proposed graph-based representation for rankingsuspicious program statements.•We conduct evaluations on two popular datasets of stu-dent programs. Experiment results show that the uniquecombination ﬁne-grained syntactic and semantic infor-mantion at AST node-level empowers FFL to achievesigniﬁcant improvements over state-of-the-art baselines.


```

## Evaluation
The training script already contain evaluation, by disabling `train()` procedure, the script will script directly to evaluation.
Copy the pretrained model into `train_dirs` in the configured `utils/utils.py` for evaluate.
### Pretrained model:
- Codeflaws node-level objective: https://drive.google.com/file/d/1E0XHg5J3wBFaGh8DYL4DmCNf-0z9z58_/view?usp=sharing
- Codeflaws statement-level objective: https://drive.google.com/file/d/1D6_YrUDCfdYqgsRwvpRA-le_MHDrFMhV/view?usp=sharing
- Prutor node-level objective: https://drive.google.com/file/d/1kfL8TpxYA491PUyKvd4nzpbFwcfqGGyY/view?usp=sharing
- NBL statement-level objective: https://drive.google.com/file/d/1Da00veDZb-445eruuE3boOoxzl1twak8/view?usp=sharing


## Others
Please note that while this is not required in our original settings, codebert pretrain file can be placed in `preprocess` folder for each AST content to be used instead of just nodetype.

Please cite the following article if you find FFL to be useful:
```
@article{Nguyen2022,
   author = {Thanh-Dat Nguyen and Thanh Le-Cong and Duc-Minh Luong and Van-Hai Duong and Xuan-Bach D Le and David Lo and Quyet-Thang Huynh},
   city = {Cyprus},
   journal = {The 38th IEEE International Conference on Software Maintenance and Evolution},
   keywords = {Graph Neural Network,Index Terms-Fault Localization,Programming Education},
   month = {11},
   title = {FFL: Fine-grained Fault Localization for Student Programs via Syntactic and Semantic Reasoning},
   url = {https://github.com/FFL2022/FFL},
   year = {2022},
}
```

